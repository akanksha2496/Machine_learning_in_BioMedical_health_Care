# -*- coding: utf-8 -*-
"""Group-9_PriyankaBoral_MT19127.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12-aHmVbBgKD-7-Vqi97Q7T6fJVE9v3hu
"""





import sys

input1 = sys.argv[1]#kaggle_train.csv
input2= sys.argv[2]#kaggle_test.csv
output1 = sys.argv[3]#sample.csv

"""## Sequential model with dense layer"""

print("Sequential model with dense layer")

import pandas as pd
#reading the training csv file using pandas
df_train = pd.read_csv(input1)
#reading the testing csv file using pandas
df_test = pd.read_csv(input2)
#assigning the train labels in a variable
target=df_train['Labels']
#dropping the labels fron input data
df_train.drop(['Labels'], inplace = True,axis=1) 
#droppung the ID from input data as it has no use
df_train.drop(['ID'], inplace = True,axis=1)
#assigning the testing ID in a variable
test_labels=df_test['ID']
#dropping the test ID from data
df_test.drop(['ID'], inplace = True,axis=1)
#splitting the data into test nad train part
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_train, target, test_size=0.20, random_state=32)

import tensorflow as tf
from tensorflow import keras
import numpy as np
#dense layers are used with different nodes abd last layer is sigmoid for binary classification
model = keras.Sequential([
    #keras.layers.Dense(250,activation='relu',input_shape=(318,)),
    #keras.layers.Dropout(0.2),
    keras.layers.Dense(150,activation='relu'),
    keras.layers.Dropout(0.1),
    keras.layers.Dense(1,activation='sigmoid')
])

from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
#Adam is used as optimiser and binary cross entropy loss
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
#checkpoint is made for checking decrease in  validation loss 
mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min',verbose=1)
#model is run on batch size 10 and 100 epoch
model.fit(X_train.values,np.asarray(y_train), epochs=100,validation_split=0.2,callbacks=[mcp_save],batch_size=10,verbose=0)

#checking the accuracy on validation data 
scores = model.evaluate(X_test, y_test, verbose=0)
print('Accuracy: %.2f' % (scores[1]*100))
#predicting on testind data
y_pred=model.predict_classes(X_test)

#MCC score on validation data
from sklearn.metrics import matthews_corrcoef
print("MCC at Sequential: ",matthews_corrcoef(y_test, y_pred))



#fitting the model on whole train data
model.fit(df_train.values,np.asarray(target), epochs=100,validation_split=0.2,callbacks=[mcp_save],batch_size=10)

#predicting on kaggle test data
y_pred=model.predict_classes(df_test)
result=[]
for i in y_pred:
  result.append(i[0])
#writing tthe output on a dataframe
data_res=pd.DataFrame()
data_res['ID']=test_labels
data_res['Labels']=result
#writing the dataframe on csv file
data_res.to_csv (r'dense(150,0.1,1).csv', index = False,header=True)

data_res.to_csv(output1,index=False)

"""## CNN"""

print("\n")
print("CNN")

import pandas as pd
#reading the training csv file using pandas
df_train = pd.read_csv(input1)
#reading the testing csv file using pandas
df_test = pd.read_csv(input2)
#assigning the train labels in a variable
target=df_train['Labels']
#dropping the labels fron input data
df_train.drop(['Labels'], inplace = True,axis=1) 
#droppung the ID from input data as it has no use
df_train.drop(['ID'], inplace = True,axis=1)
#assigning the testing ID in a variable
test_labels=df_test['ID']
#dropping the test ID from data
df_test.drop(['ID'], inplace = True,axis=1)
#splitting the data into test nad train part
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_train, target, test_size=0.20, random_state=32)

import numpy as np
#changing the dimension for fitting in CNN model
X_train = np.expand_dims(X_train, axis=2)
X_test = np.expand_dims(X_test, axis=2)

from keras.models import Sequential
from keras.layers import Dense, Conv1D, Flatten
#create model
model = Sequential()
#add model layers
model.add(Conv1D(64, kernel_size=5, activation='relu', input_shape=(318,1)))
model.add(Conv1D(30, kernel_size=5, activation='relu'))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

#compile model using accuracy to measure model performance
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#train the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100,batch_size=5)

scores = model.evaluate(X_test, y_test, verbose=0)
print('Accuracy: %.2f' % (scores[1]*100))
y_pred=model.predict_classes(X_test)
from sklearn.metrics import matthews_corrcoef,confusion_matrix
print("mathews coefficient",matthews_corrcoef(y_test, y_pred))
print("confusion matrix",confusion_matrix(y_test, y_pred))
import warnings
warnings.filterwarnings("ignore")

df_train = np.expand_dims(df_train, axis=2)
df_test = np.expand_dims(df_test, axis=2)

from keras.callbacks import ModelCheckpoint
mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min',verbose=1)
model.fit(df_train,np.asarray(target), epochs=100,validation_split=0.2,callbacks=[mcp_save],batch_size=5,verbose=0)

#predicting on kaggle test data
y_pred=model.predict_classes(df_test)
result=[]
for i in y_pred:
  result.append(i[0])
#writing tthe output on a dataframe
data_res=pd.DataFrame()
data_res['ID']=test_labels
data_res['Labels']=result
#writing the dataframe on csv file
data_res.to_csv (r'dense(150,0.1,1).csv', index = False,header=True)

data_res.to_csv(output1,index=False)

"""# LSTM:"""

print("LSTM")

"""**Import libearies that is panads and then read here the train file into the dataframe, and put in variable df:**"""

import pandas as pd
df=pd.read_csv(input1)

"""**Show the top rows of dataframe**"""

df.head()

"""**Put the values of labele column in the target variable so can use furthur, and drop the 'Labels' column from the tarin data:**"""

target=df['Labels']
df.drop(['Labels'], inplace = True,axis=1)

"""**Shape of train data is 242by319:**"""

df.shape

"""**Print the target values of all 242 rows:**"""

target

"""**this is dataframe with all columns and without taget columns:**"""

df.head()

"""**removing the ID column from the train data as it is of not use right now we can use it furthur while generating target of sample output:**"""

df.drop(['ID'], inplace = True,axis=1)

"""**This is final dataset after removing ID and Label columns:**"""

df

"""**Here in test variable all the test data is assigned in the form of dataframe**"""

test=pd.read_csv(input2)

"""**Values of ID column were assigned into the test_labels variable so can use further while prediction:**"""

test_labels=test['ID']

"""**Here ID column is dropped from test data as currently it will not used while test labels predictions:**"""

test.drop(['ID'], inplace = True,axis=1) 
test.head()

"""**This is function for Matthews_correlation_coefficient on the basis of which we decide which model is best:**"""

import math

def Mattews_Correlation_Coefficient(tp,tn,fp,fn):
  if (tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)!=0:
    MCC=((tp*tn)-(fp*fn))/((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))**(0.5)
  else:
    MCC=0
  return MCC

"""**From here LSTM is started to apply as a model into the train data. And here we tarin the model and check validation accuracy as well:**"""

from keras.layers.recurrent import LSTM
from sklearn.model_selection import StratifiedKFold
from keras.models import Sequential
import numpy as np
from keras.layers import Dense, Dropout, Activation
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.callbacks import ReduceLROnPlateau

# load test data again and upload into the dataframe:
test=pd.read_csv(input2)

# drop ID column from the test data:
test.drop(['ID'],inplace = True,axis=1)

# convert test dataframe into the numpy array so can use furthur:
test=np.asarray(test)

#define 5-fold cross validation
kfold=StratifiedKFold(n_splits=5,shuffle=True, random_state=0)
#kfold = KFold(n_splits=10)
# reshaping of df that is train data and put it on X which is numpy array
# And in X_reshape amd test_reshape we have store the shape which can use 
# furthur to reshape while training the model:
X=np.asarray(df)
X_reshape= np.reshape(X, (X.shape[0], 1, X.shape[1]))
test_reshape=np.reshape(test, (test.shape[0],1,test.shape[1]))

# type of the  X:
type(X)

# Shape of the X: train data:
X.shape

y=np.asarray(target)

"""**Here we train the model by applying LSTM where it have 2 hidden layer, of 200, 150, 16 and output is 2 by applying activation function relu relu and softmax for output.**"""

cvscores=[]
i1=[]
i=0
for train,test in kfold.split(X,y):
  i=i+1
  model = Sequential()
  model.add(LSTM((200),input_shape=(1,318),activation='relu',return_sequences=True))
  # model.add(LSTM(150,activation='relu'))
  model.add(LSTM(16,activation='relu'))
 
  model.add(Dense(2,activation='softmax'))
 
  i1.append(i)
  #compile the model
  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')
  mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_acc', mode='max')
  reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')
  #train the model
  history=model.fit(X_reshape[train], y[train], epochs=100,batch_size=5,callbacks=[earlyStopping,mcp_save],validation_data=(X_reshape[test],y[test]),verbose=0)

  #Evaluate the model
  scores = model.evaluate(X_reshape[test], y[test], verbose=0)
  print('Accuracy: %.2f' % (scores[1]*100))
  #MCC score on validation data
  cvscores.append(scores[1]*100)
  predicted_value=model.predict_classes(X_reshape[test])
  from sklearn.metrics import matthews_corrcoef,confusion_matrix
  print("mathews coefficient",matthews_corrcoef(y[test], predicted_value))
  print("confusion matrix",confusion_matrix(y[test], predicted_value))
  import warnings
  warnings.filterwarnings("ignore")


print("%.2f%% (+/-%.2f%%)" % (np.mean(cvscores),np.std(cvscores)))

"""**fit the model by using train data and then predict the output of test data.**"""

model.fit(X_reshape, y)
y_pred=model.predict_classes(test_reshape)

"""**Here label encoding is done if value greater then or equal to 1 then say it as 1 else put it as 0:**"""

yp=[]
for i in y_pred:
  if i>=1:
    yp.append(1)
  else:
    yp.append(0)





"""**Here we generate sample.csv file so we can upload in kaggle to check the output:**"""

data_res=pd.DataFrame()
data_res['ID']=test_labels
data_res['Lable']=yp
data_res.to_csv (r'LSTM3_more1.csv', index = False,header=True)

data_res.to_csv(output1,index=False)

"""## MLP"""

# from sklearn.neural_network import MLPClassifier
# clf = MLPClassifier(solver='adam', alpha=1e-5,
#                    hidden_layer_sizes=(1,318), random_state=0)

# #clf=MLPClassifier(hidden_layer_sizes=(15, ), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)


# #clf=MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5,2), random_state=1,
#                 #solver='lbfgs')

# clf.fit(X_train,y_train)

# from sklearn.model_selection import cross_val_score
# cross_val_score(clf,X_train,y_train,cv=5).mean()

# #predicting on kaggle test data
# y_pred=model.predict_classes(y_test)
# result=[]
# for i in y_pred:
#   result.append(i[0])
# #writing tthe output on a dataframe
# data_res=pd.DataFrame()
# data_res['ID']=test_labels
# data_res['Labels']=result
# #writing the dataframe on csv file
# data_res.to_csv (r'mlp.csv', index = False,header=True)

# from sklearn.metrics import matthews_corrcoef,confusion_matrix
# print("mathews coefficient",matthews_corrcoef(y_test, y_pred))
# print("confusion matrix",confusion_matrix(y_test, y_pred))
# import warnings
# warnings.filterwarnings("ignore")