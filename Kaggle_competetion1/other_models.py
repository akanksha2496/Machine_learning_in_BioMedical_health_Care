# -*- coding: utf-8 -*-
"""MLBA_A1_try.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B7Y4Je-gEx4WaeMhDmsptkQOtw7uQZD7

TEAM MEMBERS:-

Priyanka Boral(MT19127)


Reecha Kumari Giri(MT19134)


Akanksha Dewangan(MT19049)
"""

#files are loaded in data frame.
import pandas as pd
df_train = pd.read_csv('train_final_amino_acid_result.csv')
df_test = pd.read_csv('valid_final_amino_acid_result.csv')

#df_train_dipep = pd.read_csv('final_dipeptide_result_train.csv')
#df_test_dipep = pd.read_csv('final_dipeptide_result_valid.csv')

#test labels
import pandas as pd
#merging of dipeptide and amino acid 
#df_train = pd.read_csv('merged_train.csv')
#df_test = pd.read_csv('merged_test.csv')

del df_train['ID']
#test labels 
test_labels=df_test['ID']
del df_test['ID']

#train data
df_train

#label encoding to convert (1,-1) labels to (0,1) by preprocessing
from sklearn import preprocessing  
label_encoder = preprocessing.LabelEncoder() 
df_train['Type']= label_encoder.fit_transform(df_train['Type'])

#label maping of classes {'DNA': 0, 'NDNA': 1}
label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
label_mapping

#X is train data
#y is target column
X = df_train.iloc[:,:-1]
y = df_train['Type']
# split test train data where test is 20% and train is 80%
from sklearn.model_selection import train_test_split
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,shuffle=True)

#Merged data
X1 = df_train1.iloc[:,:-1]
y1 = df_train1['Type']
from sklearn.model_selection import train_test_split
X_train1, X_valid1, y_train1, y_valid1 = train_test_split(X1, y1, test_size=0.2,shuffle=True)

"""SVM Model"""

#SVM model over train data
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn import metrics
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)

svc = svm.SVC(kernel='linear', C=500, gamma=3).fit(X_train, y_train)
y_pred_svm = svc.predict(X_valid)

print('Accuracy on test set: ', metrics.accuracy_score(y_valid, y_pred_svm))

# Akku's output2.csv result is obtained on commenting X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,shuffle=True) line
# SVM Model where cross validation of K-Fold is applied
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn import svm
scaler = StandardScaler()
X = scaler.fit_transform(X)
svc = svm.SVC(kernel='linear', C=500,gamma='auto').fit(X, y)
scores = cross_val_score(svc, X, y, cv=10)
# from sklearn import metrics
# print("Accuracy:",metrics.accuracy_score(y_valid, y_pred))

#use this for svm for checking cross validation score
sum(scores)/len(scores)

#predicting for test data
svc = svm.SVC(kernel='linear', C=500, gamma=3).fit(X, y)
df_test=scaler.transform(df_test)
y_pred=svc.predict(df_test)

y_pred

# DNA=1, NDA=-1

# label encoding again where 0 is encoded to 1 and 1 is encoded to -1.
y_pred_temp=[]
for i in y_pred:
  if i==0:
    y_pred_temp.append(1)
  else:
    y_pred_temp.append(-1)

#copying to sample file
#conversion into output file with 2 column [ID,Labels]
data_res=pd.DataFrame()
data_res['ID']=test_labels
data_res['Lable']=y_pred_temp
data_res.to_csv (r'new_svm_output.csv', index = False, header=True)

"""Random Forest Model(Ensembling)"""

# Random Forest along wiht k-fold 10 cross validation.
from sklearn.ensemble import RandomForestClassifier
from sklearn import model_selection
from sklearn.metrics import accuracy_score, roc_auc_score
#cross validation
kfold = model_selection.KFold(n_splits=10, random_state=5)
clf = RandomForestClassifier(n_estimators=135, random_state=None, n_jobs=4, criterion='entropy', max_features='auto')
clf.fit(X_train, y_train)
y_pred_rf = clf.predict(X_valid)
results = model_selection.cross_val_score(clf, X_train, y_train, cv=kfold)
print(results.mean())

#print('Accuracy on test set: ', accuracy_score(y_valid, y_pred_rf))

#
y_pred=clf.predict(df_test)

print(len(y_pred))

# label encoding again where 0 is encoded to 1 and 1 is encoded to -1.
y_pred_temp_rf=[]
for i in y_pred:
  if i==0:
    y_pred_temp_rf.append(1)
  else:
    y_pred_temp_rf.append(-1)

#copying to sample file
data_res=pd.DataFrame()
data_res['ID']=test_labels
data_res['Lable']=y_pred_temp_rf
data_res.to_csv (r'output3_RF.csv', index = False,header=True)

"""MLP Classifier Model:"""

#MLP
from sklearn.neural_network import MLPClassifier
clf_mlp=MLPClassifier(alpha=1e-05, hidden_layer_sizes=(150,120), random_state=1,
            solver='lbfgs')
clf_mlp.fit(X_train, y_train)
y_pred_mlp = clf_mlp.predict(X_valid)
print('Accuracy on valid set: ', accuracy_score(y_valid, y_pred_mlp))

"""Naive Bayes Classifier Model:"""

# Naive Bayes
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_valid)
print('Accuracy on test set: ', accuracy_score(y_valid, y_pred))

"""AdaBoost Model:"""

#Adaboost
import pandas
from sklearn import model_selection
from sklearn.ensemble import AdaBoostClassifier
seed = 7
num_trees = 30
kfold = model_selection.KFold(n_splits=10, random_state=seed)
model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)
results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold)
print(results.mean())

"""ExtraTree Classifier Model:"""

import pandas
from sklearn import model_selection
from sklearn.ensemble import ExtraTreesClassifier

seed = 9
num_trees = 120
max_features = 8
kfold = model_selection.KFold(n_splits=10, random_state=seed)
model1 = ExtraTreesClassifier(n_estimators=num_trees, criterion='gini',min_samples_split=2, max_features=max_features, min_samples_leaf=1, min_weight_fraction_leaf=0, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=-1, random_state=False, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0, max_samples=None)
results = model_selection.cross_val_score(model1, X_train, y_train, cv=kfold)
print(results.mean())

model1.fit(X_train, y_train)
y_pred=model1.predict(df_test)

y_pred_temp_rf=[]
for i in y_pred:
  if i==0:
    y_pred_temp_rf.append(1)
  else:
    y_pred_temp_rf.append(-1)

#copying to sample file
data_res=pd.DataFrame()
data_res['ID']=test_labels
data_res['Lable']=y_pred_temp_rf
data_res.to_csv (r'output4_ETC.csv', index = False,header=True)



"""Voting Classifier Model:"""

#Voting classifier
import pandas
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
seed = 7
kfold = model_selection.KFold(n_splits=5, random_state=seed)
# create the sub models
estimators = []
#estimators.append(('mlp',clf_mlp))
#estimators.append(('svm',svc))
estimators.append(('randomforest',clf))
#estimators.append(('adaboost',model))
estimators.append(('Extratreeclassifier',model1))
# create the ensemble model
ensemble = VotingClassifier(estimators)
results = model_selection.cross_val_score(ensemble, X, y, cv=kfold)
print(results.mean())

"""KNN Classifier:"""

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=21)
neigh.fit(X, y)
kfold = model_selection.KFold(n_splits=10, random_state=42)
neigh.fit(X, y)
results = model_selection.cross_val_score(clf, X, y, cv=kfold)
print(results.mean())

"""XGBoost Classifier:"""

from sklearn.ensemble import GradientBoostingClassifier
xgb = GradientBoostingClassifier( n_estimators=110, max_depth=70,max_features=110)
xgb.fit(X, y)
kfold = model_selection.KFold(n_splits=10, random_state=42)
xgb.fit(X, y)
results = model_selection.cross_val_score(clf, X, y, cv=kfold)
print(results.mean())

"""Voting classifier:"""

eclf1 = VotingClassifier(estimators=[
      ('gnb', clf), ('ext', model),('rnd',rdf),('ada',model2)], voting='soft')

eclf1 = eclf1.fit(X, y)
results = model_selection.cross_val_score(eclf1, X, y, cv=kfold)
y_pred=eclf1.predict(df_test)
results.mean()

y_pred_temp_rf=[]
for i in y_pred:
  if i==0:
    y_pred_temp_rf.append(1)
  else:
    y_pred_temp_rf.append(-1)

#copying to sample file
data_res=pd.DataFrame()
data_res['ID']=test_labels
data_res['Lable']=y_pred_temp_rf
data_res.to_csv (r'output_Major_voting2.csv', index = False,header=True)

